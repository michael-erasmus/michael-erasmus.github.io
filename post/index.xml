<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on michaelerasm.us</title>
    <link>http://michaelerasm.us/post/index.xml</link>
    <description>Recent content in Posts on michaelerasm.us</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 23 Mar 2017 16:50:00 -0700</lastBuildDate>
    <atom:link href="http://michaelerasm.us/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>A machine that generates money with pandas-datareader and Prophet</title>
      <link>http://michaelerasm.us/post/money-machine/</link>
      <pubDate>Thu, 23 Mar 2017 16:50:00 -0700</pubDate>
      
      <guid>http://michaelerasm.us/post/money-machine/</guid>
      <description>

&lt;h3 id=&#34;what-is-this&#34;&gt;What is this?&lt;/h3&gt;

&lt;p&gt;This isn&amp;rsquo;t really a money machine, I&amp;rsquo;m just kidding about that, sorry.&lt;/p&gt;

&lt;p&gt;This is just a quick exploration of two awesome Python packages that I wanted to play with for a while&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://facebookincubator.github.io/prophet&#34;&gt;&lt;code&gt;Prophet&lt;/code&gt;&lt;/a&gt; for time series forecasting&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pandas-datareader.readthedocs.io/en/latest/&#34;&gt;&lt;code&gt;pandas_datareader&lt;/code&gt;&lt;/a&gt; for grabbing historic stock price data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Prophet seems like an awesome project by Facebook to make state-of-the-art time series forecasting really easy and simple. I&amp;rsquo;ve been hoping to give it a try for a while.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve also been itching to play aound with pandas-datareader, which makes it really grab data from multiple &lt;a href=&#34;https://pandas-datareader.readthedocs.io/en/latest/remote_data.html&#34;&gt;remote datasources&lt;/a&gt;, including historic stock prices from &lt;a href=&#34;https://pandas-datareader.readthedocs.io/en/latest/remote_data.html#remote-data-yahoo&#34;&gt;Yahoo! Finance&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I decided to see how Prophet does at forecasting future stock prices based on historic data. Now, before you scold me about the fact that &lt;a href=&#34;https://en.wikipedia.org/wiki/Random_walk_hypothesis&#34;&gt;what I&amp;rsquo;m doing is silly&lt;/a&gt;, I know, you can&amp;rsquo;t really use this to predict stock prices, but time series forecasting can be useful for many other things.&lt;/p&gt;

&lt;p&gt;Plus it never hurts to prove well known hypothesis to yourself, no matter how much it might seem like common knowledge.&lt;/p&gt;

&lt;p&gt;So I don&amp;rsquo;t really expect to become a millionaire quite yet, but it&amp;rsquo;s a fun little project to learn more about these awesome packages.&lt;/p&gt;

&lt;p&gt;So let&amp;rsquo;s get started!&lt;/p&gt;

&lt;h3 id=&#34;enviroment&#34;&gt;Enviroment&lt;/h3&gt;

&lt;p&gt;Docker, docker-compose and Jupyter is my preferred way of setting up a reproducible enviroment for analysis.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/michael-erasmus/money-machine/blob/master/Analysis.ipynb&#34;&gt;notebook&lt;/a&gt; for this analysis runs with &lt;a href=&#34;https://github.com/michael-erasmus/money-machine/blob/master/Dockerfile&#34;&gt;Dockerfile&lt;/a&gt; I set up using Jupyter&amp;rsquo;s &lt;a href=&#34;https://github.com/jupyter/docker-stacks/tree/master/datascience-notebook&#34;&gt;opinionated docker image&lt;/a&gt;, along with some tweaks to install the two custom packages.&lt;/p&gt;

&lt;p&gt;Once I got everything up and running, I could &lt;a href=&#34;https://github.com/michael-erasmus/money-machine/blob/master/README.md&#34;&gt;fire up a Notebook&lt;/a&gt; and get cracking.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#The usual suspects
import datetime
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

#Plus some new interesting characters
from fbprophet import Prophet
import pandas_datareader.data as web
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;our-data-source&#34;&gt;Our data source&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;pandas_datareader&lt;/code&gt; makes it incredibly easy to get historic stock market data&lt;/p&gt;

&lt;p&gt;For today we&amp;rsquo;ll just get the daily closing prices of the S&amp;amp;P 500 (SPY)&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll train on data from the start of 2000 to 2017-03-14
We&amp;rsquo;ll use the last week&amp;rsquo;s data as a holdout set to evaluate our model (up until 2017-03-21)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;start = datetime.datetime(2000, 1,1)
end = datetime.datetime(2017, 3, 21)

train = web.DataReader(&amp;quot;SPY&amp;quot;, &#39;yahoo&#39;, start, end)
compare = train.copy()
train = train[&#39;2000-01-03&#39;: &#39;2017-03-14&#39;]
train.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Open&lt;/th&gt;
      &lt;th&gt;High&lt;/th&gt;
      &lt;th&gt;Low&lt;/th&gt;
      &lt;th&gt;Close&lt;/th&gt;
      &lt;th&gt;Volume&lt;/th&gt;
      &lt;th&gt;Adj Close&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Date&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;2000-01-03&lt;/th&gt;
      &lt;td&gt;148.250000&lt;/td&gt;
      &lt;td&gt;148.250000&lt;/td&gt;
      &lt;td&gt;143.875000&lt;/td&gt;
      &lt;td&gt;145.4375&lt;/td&gt;
      &lt;td&gt;8164300&lt;/td&gt;
      &lt;td&gt;105.366938&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2000-01-04&lt;/th&gt;
      &lt;td&gt;143.531204&lt;/td&gt;
      &lt;td&gt;144.062500&lt;/td&gt;
      &lt;td&gt;139.640594&lt;/td&gt;
      &lt;td&gt;139.7500&lt;/td&gt;
      &lt;td&gt;8089800&lt;/td&gt;
      &lt;td&gt;101.246443&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2000-01-05&lt;/th&gt;
      &lt;td&gt;139.937500&lt;/td&gt;
      &lt;td&gt;141.531204&lt;/td&gt;
      &lt;td&gt;137.250000&lt;/td&gt;
      &lt;td&gt;140.0000&lt;/td&gt;
      &lt;td&gt;12177900&lt;/td&gt;
      &lt;td&gt;101.427563&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2000-01-06&lt;/th&gt;
      &lt;td&gt;139.625000&lt;/td&gt;
      &lt;td&gt;141.500000&lt;/td&gt;
      &lt;td&gt;137.750000&lt;/td&gt;
      &lt;td&gt;137.7500&lt;/td&gt;
      &lt;td&gt;6227200&lt;/td&gt;
      &lt;td&gt;99.797478&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2000-01-07&lt;/th&gt;
      &lt;td&gt;140.312500&lt;/td&gt;
      &lt;td&gt;145.750000&lt;/td&gt;
      &lt;td&gt;140.062500&lt;/td&gt;
      &lt;td&gt;145.7500&lt;/td&gt;
      &lt;td&gt;8066500&lt;/td&gt;
      &lt;td&gt;105.593338&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h3 id=&#34;can-we-forecast-future-prices&#34;&gt;Can we forecast future prices?&lt;/h3&gt;

&lt;p&gt;This is where we get to play with Prophet. First we&amp;rsquo;ll prepare our dataset in a way that Prophet likes it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.DataFrame({&#39;ds&#39;: train.index, &#39;y&#39;: train[&#39;Adj Close&#39;]}).reset_index()

df = df.drop(&#39;Date&#39;, axis=1)
df.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;ds&lt;/th&gt;
      &lt;th&gt;y&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2000-01-03&lt;/td&gt;
      &lt;td&gt;105.366938&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2000-01-04&lt;/td&gt;
      &lt;td&gt;101.246443&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2000-01-05&lt;/td&gt;
      &lt;td&gt;101.427563&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2000-01-06&lt;/td&gt;
      &lt;td&gt;99.797478&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2000-01-07&lt;/td&gt;
      &lt;td&gt;105.593338&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Now we can train our model. Since we don&amp;rsquo;t expect stock prices to really follow a weekly seasonality we&amp;rsquo;ll switch that off.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m = Prophet(weekly_seasonality=False)
m.fit(df);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To be able to make forecasts on future dates we can use a handy helper method to generate our expected input&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;future = m.make_future_dataframe(periods=365)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally we can make our prediction, using probably the coolest line of code I&amp;rsquo;ve ever written&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;forecast = m.predict(future)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;let-s-see-how-that-did&#34;&gt;Let&amp;rsquo;s see how that did!&lt;/h3&gt;

&lt;p&gt;We can use our &lt;code&gt;compare&lt;/code&gt; dataframe from earlier to see how our forecasted value for yesterday&amp;rsquo;s price compares to the actual price. We&amp;rsquo;ll also see what our model thinks the price would be in a year from now.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def evaluate(forecasted, compare):
    forecasted = forecast.set_index(&#39;ds&#39;).loc[&#39;2017-3-21&#39;][[&#39;yhat&#39;, &#39;yhat_lower&#39;, &#39;yhat_upper&#39;]]
    real = compare.loc[&#39;2017-03-21&#39;][&#39;Adj Close&#39;]
    print(&amp;quot;Yesterday&#39;s Forecast:&amp;quot; )
    print(&amp;quot;{0:.2f} lower: {1:.2f}, upper: {2:.2f}, &amp;quot;.format(forecasted[&#39;yhat&#39;], forecasted[&#39;yhat_lower&#39;], forecasted[&#39;yhat_upper&#39;]))
    print(&amp;quot;\nReal:&amp;quot; )
    print(real)

    print(&#39;\nNext year&#39;&#39;s Forecast:&#39;)

    return forecast.tail(1)[[&#39;ds&#39;,&#39;yhat&#39;, &#39;yhat_lower&#39;, &#39;yhat_upper&#39;]]

evaluate(forecasted, compare)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Yesterday&#39;s Forecast:
225.36 lower: 217.63, upper: 233.19,

Real:
233.729996

Next years Forecast:
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;ds&lt;/th&gt;
      &lt;th&gt;yhat&lt;/th&gt;
      &lt;th&gt;yhat_lower&lt;/th&gt;
      &lt;th&gt;yhat_upper&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;4690&lt;/th&gt;
      &lt;td&gt;2018-03-14&lt;/td&gt;
      &lt;td&gt;242.348516&lt;/td&gt;
      &lt;td&gt;232.2096&lt;/td&gt;
      &lt;td&gt;252.518225&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Hmm, seems to be underestimating the actual price by quite a bit, however we&amp;rsquo;re still within Prophet&amp;rsquo;s 80% confidence interval (well almost).&lt;/p&gt;

&lt;p&gt;Did I mention this isn&amp;rsquo;t acually a valid technique for making investment decisions?&lt;/p&gt;

&lt;p&gt;That being said, let&amp;rsquo;s plot our performance, which again is made pretty easy by Prophet.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;p = m.plot(forecast)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://michaelerasm.us/img/money-machine-1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Ok, so it seems like it&amp;rsquo;s doing pretty well modeling the trend in historic data, but it does seem to miss the recent rise in prices. One thing we could try is to force the model to fit the historic trend more or less closely, or as the Prophet documentation puts it, making the model more or less flexible.&lt;/p&gt;

&lt;p&gt;To do this, we can use the tweak the &lt;code&gt;changepoint_prior_scale&lt;/code&gt; parameter (I&amp;rsquo;ll chat more on changepoints a little later on). The best value I&amp;rsquo;ve found for this parameter was &lt;code&gt;0.01&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m = Prophet(weekly_seasonality=False, changepoint_prior_scale=0.01)
m.fit(df)
future = m.make_future_dataframe(periods=365)
forecast = m.predict(future)
evaluate(forecasted, compare)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Yesterday&#39;s Forecast:
225.36 lower: 217.34, upper: 233.33,

Real:
233.729996

Next years Forecast:
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;ds&lt;/th&gt;
      &lt;th&gt;yhat&lt;/th&gt;
      &lt;th&gt;yhat_lower&lt;/th&gt;
      &lt;th&gt;yhat_upper&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;4690&lt;/th&gt;
      &lt;td&gt;2018-03-14&lt;/td&gt;
      &lt;td&gt;242.348516&lt;/td&gt;
      &lt;td&gt;233.238948&lt;/td&gt;
      &lt;td&gt;252.18966&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;This makes our model overfit the data a bit more, which gets us a little closer to our real price, but we&amp;rsquo;re still not quite there yet, and our confidence intervals is also undereporting.&lt;/p&gt;

&lt;p&gt;Seriously, don&amp;rsquo;t use this to invest real money, ok?&lt;/p&gt;

&lt;h3 id=&#34;ok-so-we-didn-t-do-too-great-what-can-we-learn&#34;&gt;Ok, so we didn&amp;rsquo;t do too great, what can we learn?&lt;/h3&gt;

&lt;p&gt;Another really nice thing about Prophet is that it&amp;rsquo;s really easy to plot the components of your forecasts (trend, yearly and/or weekly seasonality). Often this is a pretty useful tool for analysis by itself.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;p = m.plot_components(forecast)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://michaelerasm.us/img/money-machine-2.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;From the top chart we can clearly see that we&amp;rsquo;ve had one of the &lt;a href=&#34;http://fortune.com/2017/03/09/stock-market-bull-market-longest/&#34;&gt;longest bull runs in history&lt;/a&gt;, which is pretty crazy, and our model thinks things can only go up from here. This is because of one of the fundamental underpinnings of time series forecasting.&lt;/p&gt;

&lt;p&gt;The market, like other time series, will have points where the general trend changes drastically. These are called &lt;a href=&#34;https://en.wikipedia.org/wiki/Change_detection&#34;&gt;change points&lt;/a&gt; and a good forecasting algorithm needs to detect change points to be able to model the trend in the data. Change points are not caused by general cycles in the data either (seasonality), which can be predicted with more accuracy.&lt;/p&gt;

&lt;p&gt;In fact, the one thing that any forecasting algorithm cannot predict is when the next change point will be, and how what effect that will have on the trend. This is also the flaw of my money machine. The market is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Random_walk_hypothesis&#34;&gt;random walk&lt;/a&gt;, and even though we know everything about historic price trends, we can&amp;rsquo;t know when trends change.&lt;/p&gt;

&lt;p&gt;What&amp;rsquo;s really interesting though is looking at yearly seasonality plot. From this plot it does seem like there is a strong seasonality in growth rates of stock prices throughout the year. We see a peak around May 1st, a decline up until late October, after which prices seem to trend up again.&lt;/p&gt;

&lt;p&gt;This is a well known trading adage to &lt;a href=&#34;https://en.wikipedia.org/wiki/Sell_in_May&#34;&gt;Sell in May and go away&lt;/a&gt;, but looking at the data, it seems to hold up quite well and might just be a good trading strategy. So maybe I have just a built a Money Machine after all!&lt;/p&gt;

&lt;p&gt;Then again, I&amp;rsquo;m sure if it was that simple we would have many more self made millionaires in the stock market. From &lt;a href=&#34;http://www.investopedia.com/terms/s/sell-in-may-and-go-away.asp#ixzz4c79c6mNF&#34;&gt;Investopedia&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;There are limitations to implementing this strategy in practice, such as added transaction costs and tax implications of the rotation in and out of equities. Another drawback is that market timing and seasonality strategies do not always work out, and the actual results may be very different from the theoretical ones.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So please don&amp;rsquo;t bet the farm quite yet!&lt;/p&gt;

&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;This was a fun little test of Prophet and pandas_datareader.&lt;/p&gt;

&lt;p&gt;pandas_datareader makes the normally tedious task of finding, downloading, reading and transforming finance data into a dataframe incredibly easy.&lt;/p&gt;

&lt;p&gt;Prophet makes time series forecasting super simple, and it works with Python and R! It feels like a great tool in my analytics toolbox and I&amp;rsquo;ve just scratched the surface of what it can do&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://facebookincubator.github.io/prophet/docs/forecasting_growth.html&#34;&gt;Logistic growth modelling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://facebookincubator.github.io/prophet/docs/trend_changepoints.html&#34;&gt;Controlling changepoint detection and confidence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://facebookincubator.github.io/prophet/docs/holiday_effects.html&#34;&gt;Specifying holidays to control for holiday effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://facebookincubator.github.io/prophet/docs/uncertainty_intervals.html&#34;&gt;Modelling uncertainty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://facebookincubator.github.io/prophet/docs/outliers.html&#34;&gt;Dealing with outliers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>What is TF-IDF? The 10 minute guide</title>
      <link>http://michaelerasm.us/post/tf-idf-in-10-minutes/</link>
      <pubDate>Fri, 17 Mar 2017 19:03:27 -0700</pubDate>
      
      <guid>http://michaelerasm.us/post/tf-idf-in-10-minutes/</guid>
      <description>

&lt;p&gt;I recently started reading up a bit on &lt;a href=&#34;http://michaelerasm.us/web/20160731164955/https://en.wikipedia.org/wiki/Tf%E2%80%93idf&#34;&gt;tf-idf&lt;/a&gt;, which stands for &lt;em&gt;term frequency-inverse document frequency&lt;/em&gt;. Tf-idf is a simple, but surprisingly powerful technique which can be used to figure out what a document is &amp;lsquo;about&amp;rsquo;. It&amp;rsquo;s often used in the fields of information retrieval and text mining.&lt;/p&gt;

&lt;h3 id=&#34;documents&#34;&gt;Documents?&lt;/h3&gt;

&lt;p&gt;First, let&amp;rsquo;s just define what I mean with document. For our purposes, a document can be thought of all the words in a piece of text, broken down by how frequently each word appears in the text.&lt;/p&gt;

&lt;p&gt;Say for example, you had a very simple document such as this quote:&lt;/p&gt;&lt;/p&gt;

&lt;blockquote&gt;
 &lt;p&gt;Just the fact that some geniuses were laughed at does not imply that all who are laughed at are geniuses. They laughed at Columbus, they laughed at Fulton, they laughed at the Wright brothers. But they also laughed at Bozo the Clown - Carl Sagan&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If we looked at the top frequency of words in the text we get this:&lt;/p&gt;

&lt;table style=&#34;width:100%&#34;&gt;  
 &lt;tr&gt;
   &lt;td&gt;&lt;b&gt;Word&lt;/b&gt;&lt;/td&gt;
   &lt;td&gt;&lt;b&gt;Frequency&lt;/b&gt;&lt;/td&gt;
  &lt;/th&gt;
  &lt;tr&gt;
    &lt;td&gt;laughed&lt;/td&gt;
    &lt;td&gt;6&lt;/td&gt;
  &lt;/tr&gt;
 &lt;tr&gt;
    &lt;td&gt;at&lt;/td&gt;
    &lt;td&gt;6&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;the&lt;/td&gt;
    &lt;td&gt;3&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;that&lt;/td&gt;
    &lt;td&gt;2&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;geniuses&lt;/td&gt;
    &lt;td&gt;2&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;are&lt;/td&gt;
    &lt;td&gt;2&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;wright&lt;/td&gt;
    &lt;td&gt;1&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;bozo&lt;/td&gt;
    &lt;td&gt;1&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;who&lt;/td&gt;
    &lt;td&gt;1&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td colspan=2 style=&#39;center&#39;&gt;...&lt;/td&gt;
  &lt;/tr&gt;
 &lt;/table&gt;

&lt;p&gt;This structure is also often referred to as a Bag of Words. Although we care about how many times a word appear in a document, we ignore the order in which words appear.&lt;/p&gt;

&lt;h3 id=&#34;term-frequency&#34;&gt;Term Frequency&lt;/h3&gt;

&lt;p&gt;Now let&amp;rsquo;s think of a way to figure out what this text is about. What are the main themes? What would be great terms to use if I wanted to search for this document? Well a very simple way of course would just to get the highest frequency words, and declare those as &amp;lsquo;important&amp;rsquo;.&lt;/p&gt;

&lt;p&gt;The word &amp;lsquo;laughed&amp;rsquo; and &amp;lsquo;at&amp;rsquo; appears a lot, and seems to be quite representative of the text, but what about &amp;lsquo;the&amp;rsquo; and &amp;lsquo;that&amp;rsquo;?&lt;/p&gt;

&lt;p&gt;At the same time the words &amp;lsquo;geniuses&amp;rsquo; and &amp;lsquo;wright&amp;rsquo; appears less frequently, but seems to be really relevant as well.&lt;/p&gt;

&lt;p&gt;Without any more information, it&amp;rsquo;s quite difficult to only use term frequency as measure of how important a word is to describe a document.&lt;/p&gt;

&lt;h3 id=&#34;inverse-document-frequency&#34;&gt;(Inverse) Document Frequency&lt;/h3&gt;

&lt;p&gt;But what if we had a whole set of documents to our disposal? This can also be called a &lt;em&gt;Corpus&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Using structure we can gleam from how frequently words appear in different documents within a corpus can give us the additional information about how common words are in general.&lt;/p&gt;

&lt;p&gt;Once we know how common or rare certain words are in general, we can weigh their importance in any specific document.&lt;/p&gt;

&lt;p&gt;So if we see a high frequency of words like &amp;lsquo;the&amp;rsquo; and &amp;lsquo;that&amp;rsquo; in a lot of documents, we can deduce that these are common words and thus consider them to be less important in any particular document. &lt;/p&gt;&lt;/p&gt;

&lt;p&gt;On the flip side, the words &amp;lsquo;geniuses&amp;rsquo;, &amp;lsquo;wright&amp;rsquo; and &amp;lsquo;bozo&amp;rsquo; will be more rare, and thus can better represent our document. This is called the document frequency, the number of documents in corpus that particular a word occurs in.&lt;/p&gt;

&lt;p&gt;Since we want rarer words to have a higher weight, we calculate the inverse of the document frequency. All of this is expressed quite simply as a ratio:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;number_of_document_in_corpus / term_document_frequency&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;putting-it-all-together&#34;&gt;Putting it all together&lt;/h3&gt;

&lt;p&gt;Once we have the term frequency and inverse document frequency statistics for any given word, we can calculate it&amp;rsquo;s tf-idf weight simply as product of the two stats.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tf_idf = term_frequency * inverse_document_frequency&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If we wanted to grab the most important word in a particular document, we can simply calculate a tf-idf score for each word and use the top scores. And that&amp;rsquo;s it, that&amp;rsquo;s all you need to use tf-idf!&lt;/p&gt;

&lt;h3 id=&#34;oh-and-just-one-more-thing-dampening&#34;&gt;Oh, and just one more thing (dampening)&lt;/h3&gt;

&lt;p&gt;There is one minor detail I failed to mention, which is that both the term frequency and inverse document frequency is often scaled logarithmically.&lt;/p&gt;

&lt;p&gt;Why would we need to do that? Well let&amp;rsquo;s say you have a document that contains the word &amp;lsquo;cat&amp;rsquo; twice. Using tf-idf you might be well find that the document is about cats. But let&amp;rsquo;s say your corpus also contains a document in which the word &amp;lsquo;cat&amp;rsquo; appears 36 times.&lt;/p&gt;

&lt;p&gt;Both these documents might be about cats, but could we say the one is 18 times &amp;lsquo;more&amp;rsquo; about cats?&lt;/p&gt;

&lt;p&gt;If you think about it, term frequency isn&amp;rsquo;t a linear indicator of how important a word is in a document. As the number of times a word appears in a document goes up, it&amp;rsquo;s relative importance starts to flatten out.&lt;/p&gt;

&lt;p&gt;A good way to represent this is by using the log10 of the term frequency instead. Since the log of 1 is zero and the log of zero is infinity, we actually use&lt;/p&gt;

&lt;p&gt;&lt;code&gt;log(tf) + 1&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We also use the log of the inverse document frequency for the same reason, since we don&amp;rsquo;t want super rare terms to be weighted too highly.&lt;/p&gt;

&lt;h3 id=&#34;can-you-show-me-some-code&#34;&gt;Can you show me some code?&lt;/h3&gt;

&lt;p&gt;Yeah of course! Here is a quick and dirty implementation of tf-idf in python, using pandas. I&amp;rsquo;m using a dataset that comes baked into scikit-learn. The &lt;a href=&#34;http://michaelerasm.us/web/20160731164955/http://scikit-learn.org/stable/datasets/twenty_newsgroups.html&#34;&gt;&lt;code&gt;fetch_20newsgroups&lt;/code&gt;&lt;/a&gt; function will return a dataset of posts from 20 usenet newsgroups.&lt;/p&gt;

&lt;p&gt;For performance reasons, I&amp;rsquo;m using using a small subset of the dataset.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
import os
import math
import re
import pandas as pd
from collections import Counter
from sklearn.datasets import fetch_20newsgroups

#get a subset of the dataset

categories = [
        &#39;alt.atheism&#39;,
        &#39;talk.religion.misc&#39;,
        &#39;comp.graphics&#39;,
        &#39;sci.space&#39;,
    ]
docs_data = fetch_20newsgroups(subset=&#39;train&#39;, categories=categories,
                                shuffle=True, random_state=42,
                                remove=(&#39;headers&#39;, &#39;footers&#39;, &#39;quotes&#39;))

#build a pandas dataframe using the filename and data of each post
docs =  pd.DataFrame({
            &#39;filename&#39; : docs_data.filenames,
            &#39;data&#39;: docs_data.data
})

#grab the corpus size(we&#39;ll use this later for IDF)
corpus_size = len(docs)

#no let&#39;s do some basic cleaning up of the text, make everything lower case and strip out all non-letters
docs[&#39;words&#39;] = docs.data.apply(lambda doc: re.sub(&amp;quot;[\W\d]&amp;quot;, &amp;quot; &amp;quot;, doc.lower().strip()).split())

#let&#39;s calculate the word frequencies for each document (Bag of words)
docs[&#39;frequencies&#39;] = docs.words.apply(lambda words: Counter(words))

#cool, now we can calculate TF, the log+1 of the frequency of each word
docs[&#39;log_frequencies&#39;] = docs.frequencies.apply(lambda d: dict([(k,math.log(v) + 1) for k, v in d.iteritems()]))

#now let&#39;s build up a lookup list of document frequencies
#first we build a vocabulary for our corpus(set of unique words)
corpus_vocab = set([word for words in docs.words for word in words])

#now use the vocabulary to find the document frequency for each word
df = lambda word: len(docs[docs.words.apply(lambda w: word in w)])
corpus_vocab_dfs = dict([(word,math.log(corpus_size / df(word))) for word in corpus_vocab])

#phew! no let&#39;s put it all together. let&#39;s calculate tf*idf for each term
tfidf = lambda tfs: dict([(k,v * corpus_vocab_dfs[k]) for k, v  in tfs.iteritems()])
docs[&#39;tfidf&#39;] = docs.log_frequencies.apply(tfidf)

#finally we can grab the top 5 weighted terms to get keywords for each document
sorted(docs.tfidf[0], key=docs.tfidf[0].get, reverse=True)[0:5]
docs[&#39;keywords&#39;] = docs.tfidf.apply(lambda t: sorted(t, key=t.get, reverse=True)[0:5])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://gist.github.com/michael-erasmus/ad16c57cf48eb95a4b63&#34;&gt;Gist&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The code is reasonably commented, so I hope its clear enough. Let me know if you had any questions.&lt;/p&gt;

&lt;h3 id=&#34;and-that-s-a-wrap&#34;&gt;And that&amp;rsquo;s a wrap!&lt;/h3&gt;

&lt;p&gt;That&amp;rsquo;s my short and hopefully clear explanation of tf-idf. The &lt;a href=&#34;http://michaelerasm.us/web/20160731164955/https://en.wikipedia.org/wiki/Tf%E2%80%93idf&#34;&gt;wikipedia page&lt;/a&gt; has a lot more useful information and theory to dig into, if that kind of stuff tickles your fancy!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Redshift UDF to find AB test significance</title>
      <link>http://michaelerasm.us/post/a-redshift-udf-to-find-ab-test-significance/</link>
      <pubDate>Mon, 28 Sep 2015 16:11:58 +0530</pubDate>
      
      <guid>http://michaelerasm.us/post/a-redshift-udf-to-find-ab-test-significance/</guid>
      <description>&lt;p&gt;I use Amazon&amp;rsquo;s Redshift every day. It&amp;rsquo;s an amazing database for data warehousing and analytics and allows you analyze huge datasets in a blazingly efficient manner using SQL.&lt;/p&gt;

&lt;p&gt;The reason why Redshift is so fast for analysis work is that unlike many other SQL databases, it uses &lt;a href=&#34;http://michaelerasm.us/web/20161021061459/https://en.wikipedia.org/wiki/Column-oriented_DBMS&#34;&gt;columnar storage&lt;/a&gt; and is highly optimized for distributing workloads across a cluster of instances.&lt;/p&gt;

&lt;p&gt;Redshift is based on PostgreSQL 8.0.2., so it&amp;rsquo;s pretty familiar to anyone who&amp;rsquo;s used Postres or any other mainstream SQL dialect before. Though there is one feature of Postgres that Redshift didn&amp;rsquo;t have until very recently, &lt;a href=&#34;http://michaelerasm.us/web/20161021061459/http://www.postgresql.org/docs/8.3/static/xfunc.html&#34;&gt;User Defined Functions&lt;/a&gt; or UDFs for short.&lt;/p&gt;

&lt;p&gt;UDF&amp;rsquo;s are really great for encapsulating common logic and also let&amp;rsquo;s you use a more expressive language to implement logic in when SQL isn&amp;rsquo;t quite cutting it.&lt;/p&gt;

&lt;p&gt;Amazon did however very recently announce an awesome new feature, &lt;a href=&#34;http://michaelerasm.us/web/20161021061459/https://aws.amazon.com/blogs/aws/user-defined-functions-for-amazon-redshift/&#34;&gt;UDF&amp;rsquo;s that can be implemented in Python&lt;/a&gt;. Once you create an embedded function  in Redshift you can use it in any SQL query in the same manner as any native built in function, and Redshift will take care of the input/output bridge between Python and SQL and running your code in a distributed manner on your cluster.&lt;/p&gt;

&lt;p&gt;There has been some really great posts about Python UDFs already, by &lt;a href=&#34;http://michaelerasm.us/web/20161021061459/https://blogs.aws.amazon.com/bigdata/post/Tx1IHV1G67CY53T/Introduction-to-Python-UDFs-in-Amazon-Redshift&#34;&gt;Amazon&lt;/a&gt;, &lt;a href=&#34;http://michaelerasm.us/web/20161021061459/http://www.looker.com/blog/amazon-redshift-user-defined-functions&#34;&gt;Looker&lt;/a&gt; and &lt;a href=&#34;http://michaelerasm.us/web/20161021061459/https://www.periscope.io/blog/redshift-user-defined-functions-python.html&#34;&gt;Periscope&lt;/a&gt; and I highly recommend having a look at those if you&amp;rsquo;re curious.&lt;/p&gt;

&lt;p&gt;Periscope has also open sourced &lt;a href=&#34;http://michaelerasm.us/web/20161021061459/https://github.com/PeriscopeData/redshift-udfs&#34;&gt;a suite of useful UDF&amp;rsquo;s&lt;/a&gt; you can readily use in your own cluster, that comes with a UDF harness you can use to manage and test UDFs&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been itching to try out writing a UDF to learn more, and with Periscope&amp;rsquo;s harness this turned out to be pretty straightforward.&lt;/p&gt;

&lt;p&gt;At Buffer we are constantly running experiments, mostly in the form of AB tests. All of the tracking data around the experiments live in Redshift, and with Looker it&amp;rsquo;s easy for us to model, filter and aggregate the data using LookML and SQL.&lt;/p&gt;

&lt;p&gt;One crucial requirement for AB testing to check &lt;a href=&#34;http://michaelerasm.us/web/20161021061459/https://en.wikipedia.org/wiki/Statistical_significance&#34;&gt;the significance&lt;/a&gt; of results.&lt;/p&gt;

&lt;p&gt;Usually an experiment will have a control and experiment group and for each group we&amp;rsquo;ll have a number of conversions. What we&amp;rsquo;re looking for is a statistical significant difference in the conversions for either group.&lt;/p&gt;

&lt;p&gt;There are a number of free online tools that let you easily work out how significant your results are, but I thought it would be great for us to be able to do this right in the database and in Looker.&lt;/p&gt;

&lt;p&gt;One of the cool things about the new Python UDF&amp;rsquo;s in Redshift is that they already ship with a bunch of libraries that are often used in data science and analytics work, such as numpy, scipy and pandas.&lt;/p&gt;

&lt;p&gt;All this meant writing a UDF to test the null-hypothesis using the &lt;a href=&#34;http://michaelerasm.us/web/20161021061459/https://en.wikipedia.org/wiki/P-value&#34;&gt;p-value&lt;/a&gt; was pretty easy to write.&lt;/p&gt;

&lt;p&gt;Here is the result:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;create or replace function experiment_result_p_value(control_size float, control_conversion float, experiment_size float, experiment_conversion float)
returns float

stable
as $$
from scipy.stats import chi2_contingency
from numpy import array
observed = array([
  [control_size - control_conversion, control_conversion],
     [experiment_size - experiment_conversion, experiment_conversion]
])
result = chi2_contingency(observed, correction=True)
chisq, p = result[:2]
return p
$$ language plpythonu;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This function uses scipy to most of the heavy lifting. All you need to pass in is the size of the control and experiment groups as well as their corresponding conversion numbers and the function will return a &lt;a href=&#34;http://michaelerasm.us/web/20161021061459/https://en.wikipedia.org/wiki/P-value&#34;&gt;p-value&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If the p-value is less than 0.05 you can reject the null-hypothesis and say there is a significant difference between the conversion rates of the the two groups.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>